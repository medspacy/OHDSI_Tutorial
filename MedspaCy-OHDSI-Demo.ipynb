{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedspaCy OHDSI Tutorial\n",
    "This notebook introduces **medspaCy**. We start with a quick overview of the goals of medspaCy and how it can be used in clinical NLP. We then go step-by-step through a typical clinical NLP workflow and show how each of the components of medspaCy can be used to etract information from clinical text.\n",
    "\n",
    "First, we'll get set up by installing some pre-trained models. If you don't already have these installed, you may have to restart your kernel before you can load them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /Users/alecchapman/opt/anaconda3/envs/medspacy-37/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /Users/alecchapman/opt/anaconda3/envs/medspacy-37/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/alecchapman/opt/anaconda3/envs/medspacy-37/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/alecchapman/opt/anaconda3/envs/medspacy-37/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.51.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/alecchapman/opt/anaconda3/envs/medspacy-37/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/alecchapman/opt/anaconda3/envs/medspacy-37/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in /Users/alecchapman/opt/anaconda3/envs/medspacy-37/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/alecchapman/opt/anaconda3/envs/medspacy-37/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: setuptools in /Users/alecchapman/opt/anaconda3/envs/medspacy-37/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (50.3.1.post20201107)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/alecchapman/opt/anaconda3/envs/medspacy-37/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/alecchapman/opt/anaconda3/envs/medspacy-37/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/alecchapman/opt/anaconda3/envs/medspacy-37/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/alecchapman/opt/anaconda3/envs/medspacy-37/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/alecchapman/opt/anaconda3/envs/medspacy-37/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/alecchapman/opt/anaconda3/envs/medspacy-37/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/alecchapman/opt/anaconda3/envs/medspacy-37/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/alecchapman/opt/anaconda3/envs/medspacy-37/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/alecchapman/opt/anaconda3/envs/medspacy-37/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/alecchapman/opt/anaconda3/envs/medspacy-37/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alecchapman/opt/anaconda3/envs/medspacy-37/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/alecchapman/opt/anaconda3/envs/medspacy-37/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download \"en_core_web_sm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/abchapman93/spacy_models/raw/master/releases/en_info_3700_i2b2_2012-0.1.0/dist/en_info_3700_i2b2_2012-0.1.0.tar.gz\n",
      "  Using cached https://github.com/abchapman93/spacy_models/raw/master/releases/en_info_3700_i2b2_2012-0.1.0/dist/en_info_3700_i2b2_2012-0.1.0.tar.gz (12.3 MB)\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/abchapman93/spacy_models/raw/master/releases/en_info_3700_i2b2_2012-0.1.0/dist/en_info_3700_i2b2_2012-0.1.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"../medspacy\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Background: Objectives, Projects, Uses, Publications\n",
    "- Quick overview of spaCy\n",
    "- Features for common clinical NLP tasks: ConText, section detection, rule-based pattern matching, UMLS linking, sentence splitting\n",
    "- Customizable: You can write your own rules for all of the rule-based components, customize for your dataset/research problem\n",
    "- Flexible: This is more about spaCy generally, but with spaCy you can drop in various trained models, use PyTorch or TF or HuggingFace, etc…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "![MedspaCy logo](https://github.com/medspacy/medspacy/blob/master/images/medspacy_logo.png?raw=true)\n",
    "\n",
    "Python is has become the dominant programming language for data science and natural language processing (NLP). Most of the largest open-source data science projects are developed in Python and some of these projects are among the most active open-source projects ever. Tensorflow had over 11,000 unique contributors in 2020 alone.\n",
    "\n",
    "Many of the largest clinical natural langauge processing projects and frameworks are in Java, so for the clinical NLP community to use the work of the large Python data science development community, adoption of Python is needed. However, many projects in Python for clinical or biomedical NLP are developed for specific projects, research groups or for redistributing models trained for specific tasks, making widespread code-reuse low.\n",
    "\n",
    "We have developed medspaCy specifically to meet this need. medspaCy is a library of tools for performing clinical NLP and text processing tasks with the popular [spaCy](spacy.io) framework. medspaCy is designed to unify a many of the most common clinical text processing algorithms (context analysis, secton detection, UMLS mapping, etc.) into one API and style.\n",
    "\n",
    "medspaCy aims to allow for the seamless integration of essential rule-based clinical NLP methods with the growing capabilities of the most popular Python libraries.\n",
    "\n",
    "## medspaCy is...\n",
    "\n",
    "### ... a toolkit\n",
    "Unlike other libraries like scispaCy and medCAT, the main goal of medspaCy is not to implement pre-trained clinical models. Instead, medspaCy is a toolkit for designing user-specific clinical NLP pipelines. medspaCy offers a number of rule-based components which allow users to easily write rules to extract specific concepts, but can be integrated with more sophisticated techniques from other sources.\n",
    "\n",
    "### ... good for prototypes and rapid development\n",
    "medspaCy facilitates rapid development by offering default configurations for all components so everything works out-of-the-box. It also works well with interactive development tools like visualization and working in jupyter notebooks.\n",
    "\n",
    "medspaCy is simple to install and requires no admin privileges to get an environment set up on a computer.\n",
    "\n",
    "### ... customizable\n",
    "All clinical data differs and no two clinical NLP tasks are the same. medspaCy components can be easily customized with user-defined rules. One of the main advantages of spaCy is its flexible architecture, which allows you to mix and match different models and components. Similarly with medspaCy, you can add components to existing pipelines, including statistical models trained using spaCy or other frameworks.\n",
    "\n",
    "### ... compatible with other spaCy projects\n",
    "medspaCy components do not add extra layers to the spaCy API, allowing medspaCy components to be used alongside other components, such as those from libraries like scispaCy or any custom components developed for a specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is medspaCy used?\n",
    "\n",
    "### medspaCy team's projects\n",
    "\n",
    "medspaCy has been used in many VA projects such as:\n",
    "- COVID-19\n",
    "- Homelessness\n",
    "- Templated document processing\n",
    "\n",
    "### Outside projects\n",
    "medspaCy already has a small community of users and developers. Some of the outside projects so far include:\n",
    "- outside project here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publications\n",
    "### Academic Publications\n",
    "* COVID-19 BSV\n",
    "* AMIA Poster\n",
    "* AMIA Tutorial 2020\n",
    "* AMIA Paper (maybe)\n",
    "* AMIA Tutorial 2021 (maybe)\n",
    "\n",
    "### Other\n",
    "* medium articles??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick overview of spaCy\n",
    "\n",
    "We will *very* briefly go over basic features of spaCy to make sure some medspaCy terminology is established.\n",
    "\n",
    "More in-depth resources for spaCy usage is available at the [project website](spacy.io) and [online course](https://course.spacy.io/en/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "The first step is always importing the library you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like medspaCy, spaCy is a primarily a toolkit and framework. It does not have any data after installing and importing it.\n",
    "\n",
    "The spaCy developers and community distribute a large variety of models for different tasks. Each model is named according to the language, use case, training source, and size.\n",
    "\n",
    "At the top of this notebook, we installed a spaCy model using this command:\n",
    "```bash\n",
    "python -m spacy download \"en_core_web_sm\"\n",
    "```\n",
    "\n",
    "`en_core_web_sm` is one of the basic spacy models: `en` English, `core` core/general use, `web` trained on internet data, `sm` in a small size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a spaCy model\n",
    "\n",
    "These spaCy distributed are simple to load. The `load` method access a registry of installed models and can load them by name.\n",
    "\n",
    "Loading the model involves opening vocabulary files, pre-trained weights, and other resouces and using them to initialize components saved in a spacy pipeline object typically named `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `en_core_web_sm` model loads a part-of-speech tagger, a dependency parser (with sentence splitting), and a named entity recognition component with the OntoNotes labels (PER, GPE, DATE, CARDINAL, etc.).\n",
    "\n",
    "Every spaCy pipeline includes a tokenizer, but it is not visible or easily altered because it is the foundation all of spaCy's other components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a spaCy pipeline\n",
    "\n",
    "`nlp` is a callable object and takes in the text to process. It applies each component in `nlp` sequentially. \n",
    "\n",
    "So in our case `tokenizer` to `tagger` to `parser` to `ner`.\n",
    "\n",
    "Because these spaCy models are designed for general English text, our example will be the first sentence of a recent [New York Times article](https://www.nytimes.com/live/2021/03/02/world/covid-19-coronavirus/biden-says-there-will-be-enough-vaccine-available-for-all-adults-by-the-end-of-may-as-johnson-johnson-makes-a-deal-to-boost-supp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"President Biden announced Tuesday that there would be \\\n",
    "enough doses of the coronavirus vaccine available for the \\\n",
    "entire adult population in the United States by the end of \\\n",
    "May, though he said it will take longer to inoculate everyone \\\n",
    "and he urged people to remain vigilant by wearing masks.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A spaCy `Doc` object is returned when processing is done. A `Doc` is just a container for the results of any spaCy pipeline. These are usually called `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the results\n",
    "\n",
    "`doc` will have certain properties that allow you to see the results of the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like `Doc`, spaCy also has containers for results at a more specific level: `Token` and `Span`. To access `Tokens` and `Spans`, `doc` is accessed like a Python list. \n",
    "\n",
    "For example, we can look at the `Token` at index 1 of `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[1].pos_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the results \n",
    "\n",
    "spaCy also includes some tools for visualization. `displaCy` is a spacy module that can display entities and dependency results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with medspaCy\n",
    "\n",
    "You can install medspaCy using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install medspacy==0.1.0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started with medspaCy, you can load a pipeline by calling `medspacy.load()`. By default, this will load the following 3 pipeline components:\n",
    "- `PyRuSHSentencizer`: Uses [PyRuSh] for clinical sentence segmentation\n",
    "- `TargetMatcher`: A rule-based concept extractor\n",
    "- `ConTextComponent`: An implementation of the [ConText] algorithm for detecting attributes like negation and temporality\n",
    "\n",
    "You can also start a medspaCy pipeline by loading `en_core_web_sm` or any other spaCy model, but keep in mind the domain limitations of components like NER using OntoNotes labels for clinical text.\n",
    "\n",
    "Throughout this notebook, we'll customize these components as well as add new ones for additional processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import medspacy\n",
    "nlp = medspacy.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"discharge_summary.txt\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like a normal spaCy model, you process a text by calling `nlp(text)`, which returns a `Doc` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Clinical NLP Tasks and medspaCy\n",
    "medspaCy is built as a modular set of **pipeline components** which handle a specific NLP task. Because of spaCy's flexible framework, you can easily add new components, including pre-trained or custom models.\n",
    "\n",
    "In this notebook, we'll walk through the following processing steps:\n",
    "- **Rule-Based Concept Extraction**: Manually define rules for extract concepts from text\n",
    "- **Statistical NER**: Use a pre-trained model for extracting clinical problems, treatments, and tests\n",
    "- **Contextual Analysis**: Assert entity attributes such as negation, temporality, and experiencer\n",
    "- **Section Detection**: Identify the structure of a note and split into individual sections\n",
    "- **Input/Output**: Implement a complete processing pipeline by reading texts, processing them, and writing structured data back to a database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-Based Concept Extraction\n",
    "In this step, we'll manually define rules to extract clinical concepts from the text.\n",
    "\n",
    "In this example, we'll use two classes provided in `medspacy.ner` for rule-based matching: the `TargetMatcher` and `TargetRule`. These expand on spaCy's native [rule-based matching](https://spacy.io/usage/rule-based-matching) and add some additional functionality.\n",
    "\n",
    "When `TargetRule` processes a doc, it adds the matched span to `doc.ents`, which contains all of the extracted entities for a doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.ner import TargetMatcher, TargetRule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher = nlp.get_pipe(\"target_matcher\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a rule for extracting entities with the `TargetRule` class. The main arguments for `TargetRule` are:\n",
    "- `literal`: An exact phrase to match in the text\n",
    "- `category`: The semantic class of the entity (ie., `ent.label_`)\n",
    "- `pattern`: An optional pattern to match rather than `literal`. As we'll see below, this can be either a list of dictionaries defining token attributes or a regular expression string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_rules = [\n",
    "    TargetRule(literal=\"abdominal pain\", category=\"PROBLEM\"),\n",
    "    TargetRule(\"stroke\", \"PROBLEM\"),\n",
    "    TargetRule(\"hemicolectomy\", \"TREATMENT\"),\n",
    "    TargetRule(\"Hydrochlorothiazide\", \"TREATMENT\"),\n",
    "    TargetRule(\"colon cancer\", \"PROBLEM\"),\n",
    "    TargetRule(\"metastasis\", \"PROBLEM\"),\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher.add(target_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.visualization import visualize_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_ent(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Rule-Based Concept Extraction\n",
    "SpaCy has powerful pattern matching which allows you to match on a list of dictionaries which define attributes for each token. See https://spacy.io/usage/rule-based-matching for spaCy's documentation and examples. Additionally, medspaCy allows matching with regular expressions on the underlying text of the doc.\n",
    "\n",
    "Let's see some examples of `TargetRules` which utilizie pattern matching. The first rule uses token patterns to match any single token where the lower-case text is either `xrt` or `radiotherapy`. The second uses regular expressions to match various forms of Type 1/Type 2 diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_rules = [\n",
    "    # Using spaCy's dictionary token patterns\n",
    "    TargetRule(\"Acetaminophen\", \"TREATMENT\",\n",
    "               pattern=[\n",
    "                   {\"LOWER\": {\"IN\": [\"acetaminophen\", \"tylenol\"]}},\n",
    "                   {\"LIKE_NUM\": True, \"OP\": \"?\"},\n",
    "                   {\"LOWER\": \"mg\", \"OP\": \"?\"}\n",
    "               ],\n",
    "              ),\n",
    "    \n",
    "    # Using regular expressions\n",
    "    TargetRule(\"diabetes\", \"PROBLEM\",\n",
    "              pattern=r\"type (i|ii|1|2|one|two) (dm|diabetes mellitus)\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher.add(pattern_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_text = \"\"\"\n",
    "    Discharge Medications: Acetaminophen 160 mg\n",
    "    Prescribed tylenol for the pain\n",
    "    74y female with type 2 dm and a recent stroke.\n",
    "    Diagnoses: Type II Diabetes Mellitus\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_doc = nlp(sm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in sm_doc.ents:\n",
    "    print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_ent(sm_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical NER\n",
    "While rule-based models are still very useful in clinical NLP, many systems are designed as **statistical model**. In this section, we'll show how to use a pre-trained model for target concept extraction instead of defining rules. We'll then add our additional components to show how medSpaCy can be used to combine statistical NLP with other rule-based components.\n",
    "\n",
    "As an example, we'll download the model below which contains a model pretrained for clinical data. This model was trained using spaCy with data from the i2b2 2012 shared task: [**\"Evaluating temporal relations in clinical text\"**](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3756273/). This model was trained on data for the first subtask in the shared task, referred to in the challenge as **\"Clinically relevant events\"**, specifically the following **clinical concepts**:\n",
    "- **Problems:** Diagnoses, signs, and symptoms\n",
    "- **Tests:** Lab and vital measurements\n",
    "- **Treatments:** Medications, procedures, and therapies\n",
    "\n",
    "We installed this model at the beginning of this notebook with `pip`:\n",
    "```bash\n",
    "pip install https://github.com/abchapman93/spacy_models/raw/master/releases/en_info_3700_i2b2_2012-0.1.0/dist/en_info_3700_i2b2_2012-0.1.0.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on how to train a model in spaCy, or how to integrate other models, see [TODO].\n",
    "\n",
    "Once we've installed the model, we'll load it by passing in the model name, **\"en_info_3700_i2b2_2012\"**, to medspaCy's `load()` function. Now, in addition to the standard pipeline components loaded above, we also have:\n",
    "- **tagger**/**parser**: A POS tagger and dependency parser taken from spaCy's standard trained english model\n",
    "- **ner**: A trained `EntityRecognizer` component trained to extract our 3 clinical classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = medspacy.load(\"en_info_3700_i2b2_2012\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what classes will be extracted by our NER\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "ner.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's reprocess our text and see what our pre-trained model extracts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_ent(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model extracted many more concepts, but missed some of the spans we defined earlier, like **\"type ii diabetes\"**. Luckily, we can combine statistical and rule-based models by adding the rules we defined to the `TargetMatcher` component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher = nlp.get_pipe(\"target_matcher\")\n",
    "target_matcher.add(target_rules)\n",
    "target_matcher.add(pattern_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_ent(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConText\n",
    "\n",
    "Clinical text often contains mentions of concepts which the patient did not actually experience. For example:\n",
    "\n",
    "- \"There is *no evidence of* **pneumonia**\"\n",
    "- \"*Mother* with **breast cancer**\"\n",
    "- \"Patient presents for *r/o* **COVID-19**\"\n",
    "\n",
    "In all of these instances, we need to use the contextual clues around the entity to assert attributes like negation, experiencer, and uncertainty.\n",
    "\n",
    "The [ConText algorithm](https://www.sciencedirect.com/science/article/pii/S1532046409000744) is a popular method for asserting attributes of entities in clinical text such as **negation**, **temporality**, and **experiencer**. ConText is implemented in medspaCy using the `ConTextComponent`, which is loaded as part of a standard model.\n",
    "\n",
    "We can inspect the modifier-entity relationships using medspaCy's `visualize_dep` function, which draws arrows between modifiers and the entities that they modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.visualization import visualize_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"There is no evidence of pneumonia.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dep(doc)\n",
    "visualize_ent(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Mother with stroke at age 82.\")\n",
    "visualize_dep(doc)\n",
    "visualize_ent(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to linking entities and modifiers, ConText also sets a number of boolean attributes indicating whether the entity is negated, experienced by someone else, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent = doc.ents[0]\n",
    "print(ent, \"is_family\", ent._.is_family)\n",
    "print(ent,  \"is_negated\", ent._.is_negated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these attributes are available in the `context_attributes` property, and you can check if any are `True` (often meaning an entity can be excluded) with the `any_context_attributes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ent._.context_attributes)\n",
    "print()\n",
    "print(\"Any context attributes:\", ent._.any_context_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced ConText\n",
    "When you load ConText in medspaCy, it comes with a default set of rules. However, you'll often need to add new rules to match your data or implement new categories.\n",
    "\n",
    "Custom modifiers can be defined using the `ConTextRule` class. This behaves very similarly to the `TargetRule` class, but has an additional argument for the **direction** the modifier moves in the sentence. This essentially defines the direction which the arrow will point to find entities to modify:\n",
    "- **\"FORWARD\"**: Start at the modifier and go towards the end of the sentence (\"There is _**no evidence of**_ ==> **\"pneumonia**\")\n",
    "- **\"BACKWARD\"**: Go towards the beginning of the sentence (\"**Colon cancer** <== _**diagnosed in 2012**_\")\n",
    "- **\"BIDIRECTIONAL\"**: Move in both directions (\"**Sepsis** <== _**vs**_ ==> **pneumonia**\")\n",
    "\n",
    "Here we'll add a **backwards** modifier to match **\"diagnosed in <YEAR>\"** in order to identify historical diagnoses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.context import ConTextRule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = nlp.get_pipe(\"context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_rule = ConTextRule(\"diagnosed in <YEAR>\", \"HISTORICAL\",\n",
    "                           direction=\"BACKWARD\",\n",
    "                          pattern=r\"(diagnosed|dx'd) in (19|20)[\\d]{2}\"\n",
    "                           \n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.add(context_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_doc = nlp(\"Colon cancer diagnosed in 2012\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dep(short_doc)\n",
    "visualize_ent(short_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see all of the rules contained in ConText, as well as the unique categories defined, in the `rules` and `categories` attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.rules[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section detection\n",
    "We are often interested in which section of a clinical note an entity occurs in. This can be useful for excluding entities from certain sections, like the past medical history or problem list, setting attributes like temporality (similar to ConText), or for extracting entities from specific sections of the note.\n",
    "\n",
    "MedSpaCy includes the `Sectionizer` class for identifying sections in a note, which adds the `sections` attribute to a `doc`, as well as similar attributes for spans and tokens. Here, we'll instantiate a `Sectionizer` and add it to our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.section_detection import Sectionizer, SectionRule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectionizer = Sectionizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(sectionizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each section, we can see the actual section title, as well as a normalized section category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for section in doc._.sections:\n",
    "    print(section.title_span, section.category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "medspaCy will visualize the sections along with entities and modifiers in gray highlighting with **<\\< \\>>** tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_ent(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the normalized section name for each entity as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent, \"-->\", ent._.section_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Section Detection\n",
    "Note structures vary widely between different EHRs and institutions, so it's important to define sections which match your note structure. The `SectionRule` defines sections to extract, and follows the same API as `TargetRule` and `ConTextRule`.\n",
    "\n",
    "Here we'll add a rule to create a **patient_demographics** section around the patient DOB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.section_detection import SectionRule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = SectionRule(\"Date of Birth:\", \"patient_demographics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectionizer.add(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_ent(nlp(text[:200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input/Output\n",
    "Finally, once we've processed a text or corpus, we'll want to save our extracted data to disk or a database. The `medspacy.io` module has utilities for converting docs to structured data.\n",
    "\n",
    "### Extracting Structured Data\n",
    "First, the `DocConsumer` will take various levels of information from a doc and generate structured data. There are four different \"levels\" allowed by the DocConsumer, and we'll extract all of them here:\n",
    "- **\"ent\"**: Extracted entities and span attributes\n",
    "- **\"context\"**: Entity-modifier pairs\n",
    "- **\"section\"**: Discrete sections of the note\n",
    "- **\"doc\"**: The text and optional custom attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.io import DocConsumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_consumer = DocConsumer(nlp, dtypes=(\"ent\", \"context\", \"section\", \"doc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DocConsumer` will add structured data as a dictionary to the `doc._.data` attribute, which contains one key for each level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(doc_consumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc._.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have `pandas` installed, you can then directly convert a doc to a dataframe, which shows the attributes extracted for each entity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.to_dataframe(\"ent\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc._.to_dataframe(\"section\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.to_dataframe(\"context\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.to_dataframe(\"doc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Writing to a Database\n",
    "As a final step, we'll write this structured data to a database. The `DbConnect`, `DbReader` and `DbWriter` classes will handle connecting to a database, creating tables, and inserting doc data for us. \n",
    "\n",
    "Currently, medspaCy database classes support `sqlite3` or `pyodbc` databases. The function below will create a simple sqlite database which includes our discharge summary and a few additional short texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_medspacy_demo_db(drop_existing=True):\n",
    "    import os\n",
    "    if drop_existing is False and os.path.exists(\"medspacy_demo.db\"):\n",
    "        print(\"File medspacy_demo.db already exists\")\n",
    "        return\n",
    "    \n",
    "    with open(\"./discharge_summary.txt\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    import sqlite3 as s3\n",
    "\n",
    "    texts = [\n",
    "        \"There is no evidence of pneumonia.\",\n",
    "        \"Her mother was diagnosed with breast cancer.\",\n",
    "        text,\n",
    "        \n",
    "    ]\n",
    "\n",
    "    conn = s3.connect(\"medspacy_demo.db\")\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS texts;\")\n",
    "    cursor.execute(\"CREATE TABLE texts (text_id INTEGER PRIMARY KEY, text NOT NULL);\")\n",
    "\n",
    "    for text in texts:\n",
    "        cursor.execute(\"INSERT INTO texts (text) VALUES (?)\", (text,))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"Created file medspacy_demo.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_medspacy_demo_db(drop_existing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll create a connection to our database using `sqlite3` and medspaCy's `DbConnect` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.io import DbConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_conn = sqlite3.connect(\"medspacy_demo.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = DbConnect(conn=sq_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define a query to load our texts and pass it into a `DbReader` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.io import DbReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in our connection and a query to read texts:\n",
    "read_query = \"\"\"\n",
    "SELECT text\n",
    "FROM texts\n",
    "\"\"\"\n",
    "reader = DbReader(conn, read_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [r[0] for r in reader.read()] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll process our texts, create a `DbWriter` object, and then write the extracted entities back to the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(nlp.pipe(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.io import DbWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = DbWriter(conn, destination_table=\"ents\", create_table=True, drop_existing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    writer.write(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a structured dataset that we can query and analyze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = sq_conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT label_, COUNT(1) FROM ents GROUP BY label_;\")\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find examples of family history\n",
    "cursor.execute(\"SELECT text, label_ FROM ents WHERE is_family = 1 LIMIT 5; \")\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find examples of family history\n",
    "cursor.execute(\"SELECT text, label_ FROM ents WHERE text LIKE '%cancer%' LIMIT 5; \")\n",
    "cursor.fetchall()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future work\n",
    "- Next Steps: spaCy v3, documentation, trained models/pipelines, optimization, support for other natural languages …"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
